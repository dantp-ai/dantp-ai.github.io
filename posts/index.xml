<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on The Reinforced Path</title>
    <link>https://dantp-ai.github.io/posts/</link>
    <description>Recent content in Posts on The Reinforced Path</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2022-2024</copyright>
    <lastBuildDate>Thu, 28 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://dantp-ai.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quick introduction to RLHF for fine-tuning LLMs to better match human preferences</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/rlhf_brief/</link>
      <pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/rlhf_brief/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Reinforcement learning from human feedback (RLHF) is a fine-tuning technique &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; to align LLM outputs to human preferences.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;RLHF consists of four main steps &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;LLM pre-training $\rightarrow$ Base model&lt;/li&gt;&#xA;&lt;li&gt;Supervised fine-tuning (SFT) $\rightarrow$ Instruct-tuned model (IM)&lt;/li&gt;&#xA;&lt;li&gt;Reward model training $\rightarrow$ Reward model (RM)&lt;/li&gt;&#xA;&lt;li&gt;LLM policy optimization $\rightarrow$ Policy model (PM)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Steps 2-3 can be iterated continuously by using the current best policy model to get a better instruct-tuned model, which is used to get a better reward model, which is used to improve the policy model, and so on.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;why&#34;&gt;Why&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RLHF has been useful in reducing responses related to toxicity, bias, and harmfulness by using the preference signal of a well-intentioned and unbiased human feedback &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;how&#34;&gt;How&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;To explain how it works, let&amp;rsquo;s go through each step in more detail, noting he input, output, model, and data used at each step.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Curvature explains loss of plasticity</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/lewandowski2023curvature/</link>
      <pubDate>Fri, 29 Mar 2024 15:37:21 +0100</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/lewandowski2023curvature/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Curvature explains loss of plasticity&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Alex Lewandowski, Haruto Tanaka, Dale Schuurmans, Marlos C. Machado&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.00246.pdf&#34;&gt;https://arxiv.org/pdf/2312.00246.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This paper presents empirical evidence that neural networks lose curvature directions during training in a continual learning setting, resulting in a loss of plasticity in learning. The loss of curvature (defined in the &lt;a href=&#34;#measuring-curvature-to-establish-an-empirical-relationship-to-plasticity&#34;&gt;Sec. below&lt;/a&gt;) is measured by the empirical effective rank of an approximation to the Hessian of the parameters.&lt;/li&gt;&#xA;&lt;li&gt;This study also provides simple counter-examples to demonstrate that previous explanations of loss of plasticity are inconsistent with the situations in which such a loss should occur.&lt;/li&gt;&#xA;&lt;li&gt;Finally, the authors developed a simple regularizer that maintains the parameters distribution close to the initialization distribution, while also ensuring that the curvature is not significantly reduced.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;why&#34;&gt;Why&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;To achieve the ambitious goal of AI, we need to develop agents that can learn &lt;em&gt;continually&lt;/em&gt; and adapt to changes in data distribution without external intervention.&lt;/li&gt;&#xA;&lt;li&gt;Whilst there are many existing explanations drawn empirically of why loss of plasticity occurs in neural networks it is not clear what the root cause is.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;how&#34;&gt;How&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: The text describes an empirical relationship between the loss of curvature and the loss of plasticity. This relationship is more consistent with different situations than existing explanations. A simple regularizer is developed based on this curvature-plasticity relationship. It preserves the curvature of the Hessian matrix of parameters by penalizing the distribution of parameters if it lies too far away from the distribution of the parameters at initialization. This regularizer allows for a more generous departure from the initialization distribution compared to existing regularization methods, such as weight decay.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Auxiliary task discovery through generate-and-test</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/pmlr-v232-rafiee23a/</link>
      <pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/pmlr-v232-rafiee23a/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Auxiliary task discovery through generate-and-test&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Banafsheh Rafiee, Sina Ghiassian, Jun Jin, Richard Sutton, Jun Luo, Adam White&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://proceedings.mlr.press/v232/rafiee23a.html&#34;&gt;https://proceedings.mlr.press/v232/rafiee23a.html&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This paper proposes a new way to measure how useful auxiliary tasks are by looking at the features they create and how much they contribute to the main task.&lt;/li&gt;&#xA;&lt;li&gt;The test to measure usefulness follows the general generate-and-test framework.&lt;/li&gt;&#xA;&lt;li&gt;In addition to the usefulness tester for the auxiliary task, a new generator is developed based on the feature-attainment subtasks introduced by Sutton (2023) &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;why&#34;&gt;Why&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Previously, humans have defined auxiliary subtasks beforehand. However, this is not a scalable solution as it requires a significant amount of domain knowledge in complex environments. Moreover, determining what is essential to be learned in advance may not always be possible, and such subtasks may hinder learning in a non-stationary continual learning setting.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;how&#34;&gt;How&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Auxiliary subtasks (defined as GVFs) are discovered by continually generating them randomly (or through feature-attainment &lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) and testing to replace those that are less useful (based on their feature value and the weights contribution to the main task).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/schenk2022hybrid/</link>
      <pubDate>Thu, 23 Nov 2023 22:29:41 +0100</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/schenk2022hybrid/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Michael Schenk, Elías F. Combarro, Michele Grossi, Verena Kain, Kevin Shing Bruce Li, Mircea-Marian Popa, and Sofia Vallecorsa&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/2058-9565/ad261b/meta&#34;&gt;https://iopscience.iop.org/article/10.1088/2058-9565/ad261b/meta&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;The authors of this paper explore whether free energy-based reinforcement learning (FERL) with quantum Boltzmann machines (QBM) algorithms are more sample efficient than classical RL algorithms in continuous action spaces. Previous research has shown that indeed FERL-QBM algorithms are more sample efficient than classical RL algorithms in environments with discrete action spaces.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Planning with expectation models</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/wan2019planning/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/wan2019planning/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Planning with expectation models&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Yi Wan, Zaheer Abbas, Adam White, Martha White, Richard S. Sutton&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.01191.pdf&#34;&gt;https://arxiv.org/pdf/1904.01191.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;The papers demonstrates how to use expectation models in a sound way for&#xA;planning in model-based reinforcement learning (MBRL), particularly in&#xA;stochastic environments.&lt;/p&gt;&#xA;&lt;p&gt;One important insight is that planning with an expectation model is just&#xA;as expressive as with a distribution model, provided that the state&#xA;value function has a linear parameterization in the state feature&#xA;function. However, the mapping of observation-to-state-feature function&#xA;may still be non-linear.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss of plasticity in continual deep reinforcement learning</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/abbas2023loss/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/abbas2023loss/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Loss of plasticity in continual deep reinforcement learning&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, Marlos C. Machado&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.07507.pdf&#34;&gt;https://arxiv.org/pdf/2303.07507.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;The study shows how continual deep reinforcement learning (RL) experiences a significant loss of plasticity. By carefully and thoroughly examining three statistics - weight change, gradient norm, and neural network activations - the authors prove that traditional value-based deep RL methods in DQN and Rainbow are inadequate for continual learning. The benchmark for continual learning is a sequence of Atari 2600 games.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss of Plasticity in Deep Continual Learning</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/dohare2023loss/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/dohare2023loss/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Loss of Plasticity in Deep Continual Learning&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Shibhansh Dohare, J. Fernando Hernandez-Garcia, Parash Rahman, Richard S. Sutton, A. Rupam Mahmood&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.13812v2.pdf&#34;&gt;https://arxiv.org/pdf/2306.13812v2.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;This paper presents an extensive and systematic empirical study that&#xA;proves standard deep learning systems fail to keep learning in a&#xA;continual learning environment. It explains some of the core reasons for&#xA;this problem of loss of plasticity, and proposes a natural extension to&#xA;the backpropagation algorithm that can reliably maintain plasticity for&#xA;continual learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reinforcement learning with unsupervised auxiliary tasks</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/jaderberg2016reinforcement/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/jaderberg2016reinforcement/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Reinforcement learning with unsupervised auxiliary tasks&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.05397.pdf&#34;&gt;https://arxiv.org/pdf/1611.05397.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;The paper examines the concept of maximizing &amp;ldquo;pseudo-rewards&amp;rdquo;, in addition to the main reward, derived from different environment signals. The objective is to improve certain parts of the agents&amp;rsquo; feature representation. This, in turn, will help the main policy to reward states more effectively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/haarnoja2018soft/</link>
      <pubDate>Thu, 01 Jun 2023 01:45:34 +0100</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/haarnoja2018soft/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf&#34;&gt;https://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Off-policy actor-critic deep RL algorithm with &lt;em&gt;stochastic&lt;/em&gt; actor for continuous state and action space maximizing the expected reward plus the expected &lt;em&gt;entropy&lt;/em&gt; of the target policy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reinitializing the moments of the optimizer during learning (Jax)</title>
      <link>https://dantp-ai.github.io/posts/software/reinit_moments_opt/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/software/reinit_moments_opt/</guid>
      <description>&lt;p&gt;Reinitializing the weights of a neural network has been shown to be effective for continual learning (cf. CBP algorithm &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;). In this short guide, I show how to update the moments of a momentum-based optimizer in Jax.&lt;/p&gt;&#xA;&lt;p&gt;Here I reinitialized &lt;em&gt;all&lt;/em&gt; components of the two moments, not just those corresponding to low utility hidden units, as CBP does.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s consider a simple regression problem with multivariate inputs in three dimensions and scalar targets. We minimize the squared error using &lt;em&gt;stochastic&lt;/em&gt; momentum-based gradient descent. Specifically, we use the well-established Adam optimizer to update the parameters of a linear layer, processing a single example at each time step, for a total of 1000 steps.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An emphatic approach to the problem of off-policy temporal-difference learning</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/sutton2016emphatic/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/sutton2016emphatic/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;An emphatic approach to the problem of off-policy temporal-difference learning&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Richard S. Sutton, A. Rupam Mahmood, Martha White&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume17/14-488/14-488.pdf&#34;&gt;https://www.jmlr.org/papers/volume17/14-488/14-488.pdf&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Introduces a new family of Temporal-Differences (TD) methods that improve on the classic TD(λ) methods by naturally extending them with the notion of emphasizing and de-emphasizing states based on intrinsic interest in them and the interest that follows from past visited states.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Recent Developments in Emphatic Temporal-Differences Methods and Emphatic Weightings</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/mahmood2015emphatic/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/mahmood2015emphatic/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;p&gt;I have recently read some of the work that has been done on emphatic temporal-differences (ETD) methods, and more generally on the use of emphatic weightings. Below I briefly summarize my understanding of the authors&amp;rsquo; findings, and offer some of my own opinions where I think they are appropriate.&lt;/p&gt;&#xA;&lt;p&gt;ETD methods have been proposed to address the long-standing problem of instability of &lt;em&gt;off-policy&lt;/em&gt; temporal-differences (TD) methods under linear function approximation (Sutton, Mahmood, and White 2016 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;). The emphatic algorithm elegantly extends TD by reweighting updates in a special way. Specifically, the emphatic weights are derived from an initially specified interest in states, to which a discounted &lt;em&gt;follow-on&lt;/em&gt; trace of past interest from previous states is cumulatively added at each time step, and the update is scaled by this factor. The authors extended the algorithm to its most general case, including state-dependent bootstrapping, state-dependent termination, and state-dependent interest functions for &lt;em&gt;linear&lt;/em&gt; function approximation, and showed that its updates are stable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/dosovitskiy2020image/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/dosovitskiy2020image/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Authors&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Link&lt;/strong&gt;:&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.11929.pdf&#34;&gt;https://arxiv.org/pdf/2010.11929.pdf&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;overview&#34;&gt;Overview&lt;/h4&gt;&#xA;&lt;p&gt;This week I took a closer look at the ViT paper which contains some interesting experiments on how learning in the Transformer model scales with increasing computer power and data &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A review of different Transformers-based algorithms and methods</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/vaswani2017attention/</link>
      <pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/vaswani2017attention/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;p&gt;In this blog I review some of the research in the area of area of Transformers models. The article is meant to be written at a high level. For more technical details please refer to the references mentioned.&lt;/p&gt;&#xA;&lt;h3 id=&#34;recap-transformers&#34;&gt;Recap: Transformers&lt;/h3&gt;&#xA;&lt;p&gt;First, I briefly explain what Transformers are, what some typical tasks are that these models aim to solve, and go through some of their architectural components and fundamental architectures. This brief recap is largely based on the wonderful formal presentation of Transformers by Phuong &amp;amp; Hutter (2022). &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to set up a Python debugger in VS Code for pytest</title>
      <link>https://dantp-ai.github.io/posts/software/howto_python_debugger_vscode_pytest/</link>
      <pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/software/howto_python_debugger_vscode_pytest/</guid>
      <description>&lt;h3 id=&#34;running-and-debugging-a-test-module-with-pytest-using-vs-code&#34;&gt;Running and debugging a test module with pytest using VS Code&lt;/h3&gt;&#xA;&lt;p&gt;You may be wondering how to set up the Python debugger in VS Code to debug pytest tests. Let us assume that you have opened a Python project in VS Code. In the following post, I specify some shortcuts to access certain UI elements of VS Code. These keyboard shortcuts are for MacOS. You can easily find the counterparts for the other operating systems with a quick web search. Also, I am going to reference several UI elements of VS Code as defined in their official documentation, which you can access here: &lt;a href=&#34;https://code.visualstudio.com/docs/getstarted/userinterface&#34;&gt;https://code.visualstudio.com/docs/getstarted/userinterface&lt;/a&gt;. Let&amp;rsquo;s get started:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

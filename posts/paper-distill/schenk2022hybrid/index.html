<!doctype html>
<html lang="en"><head>
    <title>Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines</title>
    
    <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
        </script>
</head>
<body>
</body>
</html>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="" />

    
    
    
    <link rel="stylesheet" href="../../../css/theme.min.css">

    
    
    
    
    <link rel="stylesheet" href="../../../css/custom.min.css">
    

    
</head>
<body>
        <div id="content" class="mx-auto"><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1">
    <div class="row">
        
        <div class="col-sm-4 col-12 text-sm-right text-center pt-sm-4">
            <a href="../../../" class="text-decoration-none">
                <img id="home-image" class="rounded-circle"
                    
                        
                            src="../../../images/avatar.png"
                        
                    
                />
            </a>
        </div>
        <div class="col-sm-8 col-12 text-sm-left text-center">
        
            <h2 class="m-0 mb-2 mt-4">
                <a href="../../../" class="text-decoration-none">
                    
                        Dan
                    
                </a>
            </h2>
            <p class="text-muted mb-1">
                
                    AI Engineer
                
            </p>
            <ul id="nav-links" class="list-inline mb-2">
                
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../../" title="About">About</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white active" href="../../../posts/" title="Posts">Posts</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../../categories/" title="Categories">Categories</a>
                    </li>
                
            </ul>
            <ul id="nav-social" class="list-inline">
                
            </ul>
        </div>
    </div>
    <hr />
</header>
<div class="container">

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
        </script>
</head>
<body>
</body>
</html>

    <div class="pl-sm-2">
        <div class="mb-3">
            <h3 class="mb-0">Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines</h3>
            
            <small class="text-muted">Published November 23, 2023</small>
        </div>

        <article>
            <p><em><ins>Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.</ins></em></p>
<br>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>Title</strong>:</td>
          <td style="text-align: left">Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Authors</strong>:</td>
          <td style="text-align: left">Michael Schenk, El√≠as F. Combarro, Michele Grossi, Verena Kain, Kevin Shing Bruce Li, Mircea-Marian Popa, and Sofia Vallecorsa</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Link</strong>:</td>
          <td style="text-align: left"><a href="https://iopscience.iop.org/article/10.1088/2058-9565/ad261b/meta">https://iopscience.iop.org/article/10.1088/2058-9565/ad261b/meta</a></td>
      </tr>
  </tbody>
</table>
<h4 id="what">What</h4>
<hr>
<p>The authors of this paper explore whether free energy-based reinforcement learning (FERL) with quantum Boltzmann machines (QBM) algorithms are more sample efficient than classical RL algorithms in continuous action spaces. Previous research has shown that indeed FERL-QBM algorithms are more sample efficient than classical RL algorithms in environments with discrete action spaces.</p>
<h4 id="why">Why</h4>
<hr>
<p>CERN currently tunes some of its control systems manually. This paper examines the potential of quantum-based RL algorithms to train sample-efficient RL agents that can control these systems specifically for proton beam line steering. This minimizes the use of the beam line in the reactor, saving energy and enabling other experiments to run in the collider.</p>
<h4 id="how">How</h4>
<hr>
<blockquote>
<p><strong>TL;DR</strong>: The first study compared classical deep Q-learning and FERL-QBM-based algorithms for discrete action and continuous state spaces. The latter algorithm was found to be more sample efficient.<br>
The second study compares the classical DDPG algorithm (also known as deep Q-learning for continuous action spaces) with the newly developed hybrid actor-critic algorithm. The hybrid algorithm features a FERL-QBM-based critic and a classical DDPG-based actor. The latter is proven to be more sample efficient.</p>
</blockquote>
<h4 id="study-a-ferl-q-learning-with-continuous-state-space">Study A: FERL Q-learning with continuous state space</h4>
<p>In the first study, the state and action space are both one-dimensional. Initially, the classical Q-learning and FERL-QBM are evaluated for a discrete state-action space. Then, the setting is analyzed also for continuous state-action spaces.</p>
<p>The comparison of the two RL-based methods is ran with and without experience replay buffer.</p>
<p>The FERL method performed significantly better than deep Q-learning with a factor of 400 in improvements in sample efficiency. Moreover the ordering of the two methods was maintained with and without experience replay buffer.</p>
<h4 id="study-b-hybrid-a-c-scheme">Study B: hybrid A-C scheme</h4>
<p>The second study evaluated DDPG (a classical actor-critic) and hybrid actor-critic (Hybrid A-C) methods in a 10-dimensional state and action space.
The algorithms were trained in simulation and evaluated on a real device. Evaluation performance in simulation was almost identical to that on the real device.
Hybrid A-C was superior to DDPG but the advantage in performance is not statistically significant.</p>
<p>QBM-based methods can be more challenging to deploy due to their complexity and hardware dependence. However, the hybrid actor-critic approach has an advantage over previous FERL-QBM methods. The critic is QBM-based, but it is not necessary for inference. Only the actor, a classical policy network, is used, making deployment easier.</p>
<h4 id="thoughts">Thoughts</h4>
<hr>
<ul>
<li>FERL-QBM-based methods showed better performance in both discrete and continuous state-action spaces. However, in the latter case, the advantage over classical A-C methods was not significant. Further studies are required to determine if this holds true in other environments.</li>
<li>It would be intriguing to test SAC on the beam line steering task in the AWAKE environment and compare it to hybrid A-C (with SAC-based actor).</li>
<li>CrossQ is a method recently proposed <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> that replaces target networks in DQN-based RL algorithms with batch normalization to improve sample efficiency.
<ul>
<li>Can CrossQ be used as a hybrid A-C with FERL, and how does it compare to its classical counterpart?</li>
</ul>
</li>
</ul>
<h4 id="references">References</h4>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Bhatt, A., Palenicek, D., Belousov, B., Argus, M., Amiranashvili, A., Brox, T., &amp; Peters, J. (2019). CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. arXiv preprint arXiv:1902.05605.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

        </article>
    </div>

            </div>
        </div><footer class="text-center pb-1">
    <small class="text-muted">
        &copy; 2022-2024
        <br>
        Built with <a href="https://gohugo.io/" target="_blank">Hugo</a>
        based on <a href="https://github.com/austingebauer/devise" target="_blank">Devise</a>
        theme from <a href="https://github.com/austingebauer/devise" target="_blank">A. Gebauer.</a>
    </small>
</footer>
</body>
</html>

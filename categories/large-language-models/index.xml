<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models on The Reinforced Path</title>
    <link>https://dantp-ai.github.io/categories/large-language-models/</link>
    <description>Recent content in Large Language Models on The Reinforced Path</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2022-2024</copyright>
    <lastBuildDate>Thu, 28 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://dantp-ai.github.io/categories/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quick introduction to RLHF for fine-tuning LLMs to better match human preferences</title>
      <link>https://dantp-ai.github.io/posts/paper-distill/rlhf_brief/</link>
      <pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://dantp-ai.github.io/posts/paper-distill/rlhf_brief/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;ins&gt;Any errors or misinterpretations presented here are solely my own and do not reflect the views of the original authors.&lt;/ins&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;h4 id=&#34;what&#34;&gt;What&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Reinforcement learning from human feedback (RLHF) is a fine-tuning technique &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; to align LLM outputs to human preferences.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;RLHF consists of four main steps &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;LLM pre-training $\rightarrow$ Base model&lt;/li&gt;&#xA;&lt;li&gt;Supervised fine-tuning (SFT) $\rightarrow$ Instruct-tuned model (IM)&lt;/li&gt;&#xA;&lt;li&gt;Reward model training $\rightarrow$ Reward model (RM)&lt;/li&gt;&#xA;&lt;li&gt;LLM policy optimization $\rightarrow$ Policy model (PM)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Steps 2-3 can be iterated continuously by using the current best policy model to get a better instruct-tuned model, which is used to get a better reward model, which is used to improve the policy model, and so on.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;why&#34;&gt;Why&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RLHF has been useful in reducing responses related to toxicity, bias, and harmfulness by using the preference signal of a well-intentioned and unbiased human feedback &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;how&#34;&gt;How&lt;/h4&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;To explain how it works, let&amp;rsquo;s go through each step in more detail, noting he input, output, model, and data used at each step.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
